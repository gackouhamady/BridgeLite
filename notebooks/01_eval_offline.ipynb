{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee51686f",
   "metadata": {},
   "source": [
    "\n",
    "# BridgeLite — Offline Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the exported model bundle on **train** and **production_sim** datasets, produces:\n",
    "- macro-F1, accuracy, coverage@τ\n",
    "- confusion matrices (saved as PNGs)\n",
    "- a τ sweep (F1 vs coverage) plot to help choose the operating threshold\n",
    "- a qualitative sample of predictions\n",
    "\n",
    "> Expected repo layout:\n",
    "> - `app/model_sklearn.pkl` (bundle with `vectorizer`, `model`, `label_encoder`)\n",
    "> - `data/transactions_mock.csv`, `data/production_sim.csv`\n",
    "> - `app/preproc.py` that defines `preprocess_record`\n",
    "\n",
    "**Tip:** If running inside Docker, ensure these files exist inside the container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d758e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Import your preprocessor from the app package\n",
    "from app.preproc import preprocess_record\n",
    "\n",
    "# Save plots into repo-relative paths as well\n",
    "REPORT_DIR = Path(\"reports/eval\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Environment OK. Reports will be saved to:\", REPORT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Configuration ----\n",
    "MODEL_PATH = Path(\"app/model_sklearn.pkl\")\n",
    "TRAIN_CSV  = Path(\"data/transactions_mock.csv\")\n",
    "PROD_CSV   = Path(\"data/production_sim.csv\")\n",
    "\n",
    "TAU = 0.6            # operating threshold for coverage calculation\n",
    "N_SAMPLES_QUAL = 12  # qualitative sample size\n",
    "\n",
    "assert MODEL_PATH.exists(), f\"Missing model bundle: {MODEL_PATH}\"\n",
    "assert TRAIN_CSV.exists(), f\"Missing train CSV: {TRAIN_CSV}\"\n",
    "assert PROD_CSV.exists(),  f\"Missing prod CSV:  {PROD_CSV}\"\n",
    "\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "vec = bundle[\"vectorizer\"]\n",
    "clf = bundle[\"model\"]\n",
    "le  = bundle[\"label_encoder\"]\n",
    "labels = list(le.classes_)\n",
    "\n",
    "print(\"Loaded bundle with classes:\", labels[:10], \"...\" if len(labels)>10 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df037fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_text(row: pd.Series) -> str:\n",
    "    \"\"\"Build the exact text the model expects: masked + op/mcc tokens.\"\"\"\n",
    "    pre = preprocess_record({\"raw_label\": row.get(\"raw_label\", \"\"), \"channel\": row.get(\"channel\", None)})\n",
    "    op = pre.operation_type or \"unknown\"\n",
    "    mcc = row.get(\"mcc\", \"\")\n",
    "    mcc_tok = f\"mcc:{int(mcc)}\" if str(mcc).strip().isdigit() else \"mcc:none\"\n",
    "    return f\"{pre.masked} op:{op} {mcc_tok}\"\n",
    "\n",
    "\n",
    "def predict_batch(texts: list[str]):\n",
    "    X = vec.transform(texts)\n",
    "    proba = clf.predict_proba(X)\n",
    "    idx = np.argmax(proba, axis=1)\n",
    "    pred = le.inverse_transform(idx)\n",
    "    pmax = proba.max(axis=1)\n",
    "    return pred, pmax, proba\n",
    "\n",
    "\n",
    "def eval_split(df: pd.DataFrame, tau: float = 0.6, name: str = \"eval\"):\n",
    "    texts = df.apply(build_text, axis=1).astype(str).tolist()\n",
    "    y_true = None\n",
    "    if \"category_label\" in df.columns:\n",
    "        y_true = le.transform(df[\"category_label\"].astype(str).values)\n",
    "    pred, pmax, proba = predict_batch(texts)\n",
    "    if y_true is not None:\n",
    "        y_pred = le.transform(pred)\n",
    "        f1 = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "        acc = float(accuracy_score(y_true, y_pred))\n",
    "    else:\n",
    "        f1, acc = None, None\n",
    "    coverage = float(np.mean(pmax >= tau))\n",
    "    return {\n",
    "        \"pred\": pred, \"pmax\": pmax, \"proba\": proba, \"y_true\": y_true,\n",
    "        \"f1\": f1, \"acc\": acc, \"coverage\": coverage, \"name\": name\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes: list[str], title: str, out_png: Path):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks(range(len(classes)))\n",
    "    ax.set_yticks(range(len(classes)))\n",
    "    ax.set_xticklabels(classes, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(classes)\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out_png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "res_train = eval_split(df_train, tau=TAU, name=\"train\")\n",
    "print({k: res_train[k] for k in [\"f1\",\"acc\",\"coverage\"]})\n",
    "\n",
    "if res_train[\"y_true\"] is not None:\n",
    "    y_pred_train = le.transform(res_train[\"pred\"])\n",
    "    png = REPORT_DIR / \"confusion_train.png\"\n",
    "    plot_confusion(res_train[\"y_true\"], y_pred_train, labels, \"Confusion (train)\", png)\n",
    "    print(\"Saved:\", png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea38022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_prod = pd.read_csv(PROD_CSV)\n",
    "print(\"Prod shape:\", df_prod.shape)\n",
    "res_prod = eval_split(df_prod, tau=TAU, name=\"prod\")\n",
    "print({k: res_prod[k] for k in [\"f1\",\"acc\",\"coverage\"]})\n",
    "\n",
    "if res_prod[\"y_true\"] is not None:\n",
    "    y_pred_prod = le.transform(res_prod[\"pred\"])\n",
    "    png = REPORT_DIR / \"confusion_prod.png\"\n",
    "    plot_confusion(res_prod[\"y_true\"], y_pred_prod, labels, \"Confusion (prod)\", png)\n",
    "    print(\"Saved:\", png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6099dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "taus = np.linspace(0.4, 0.9, 26)  # step 0.02\n",
    "f1s, covs = [], []\n",
    "# Use prod if it has labels, else train\n",
    "target = res_prod if res_prod[\"y_true\"] is not None else res_train\n",
    "\n",
    "for t in taus:\n",
    "    cov = float(np.mean(target[\"pmax\"] >= t))\n",
    "    if target[\"y_true\"] is not None:\n",
    "        idx = np.argmax(target[\"proba\"], axis=1)\n",
    "        y_pred = idx  # doesn't change with tau; tau only affects coverage\n",
    "        f1 = float(f1_score(target[\"y_true\"], y_pred, average=\"macro\"))\n",
    "    else:\n",
    "        f1 = np.nan\n",
    "    covs.append(cov); f1s.append(f1)\n",
    "\n",
    "best_idx = int(np.nanargmax([f for f, c in zip(f1s, covs) if c >= 0.90] or [np.nan]))\n",
    "best_tau = float(taus[best_idx]) if f1s else None\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(taus, covs, label=\"coverage\")\n",
    "ax.plot(taus, f1s, label=\"macro-F1\")\n",
    "ax.set_title(\"τ sweep — coverage vs macro-F1\")\n",
    "ax.set_xlabel(\"τ\")\n",
    "ax.set_ylabel(\"value\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "out_png = REPORT_DIR / \"tau_sweep.png\"\n",
    "fig.savefig(out_png, dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "summary_tau = {\"taus\": taus.tolist(), \"coverage\": covs, \"macro_f1\": f1s, \"recommended_tau_if_cov90\": best_tau}\n",
    "print(\"Saved τ sweep:\", out_png, \"  recommended τ (coverage≥0.90):\", best_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show some qualitative examples with predictions\n",
    "sample_df = (df_prod if len(df_prod) else df_train).sample(min(N_SAMPLES_QUAL, len(df_prod) or len(df_train)), random_state=7)\n",
    "texts = sample_df.apply(build_text, axis=1).astype(str).tolist()\n",
    "pred, pmax, _ = predict_batch(texts)\n",
    "out = sample_df.copy()\n",
    "out[\"pred\"] = pred\n",
    "out[\"conf\"] = pmax\n",
    "if \"category_label\" in out.columns:\n",
    "    out[\"true\"] = out[\"category_label\"]\n",
    "display_cols = [c for c in [\"raw_label\",\"channel\",\"mcc\",\"true\",\"pred\",\"conf\"] if c in out.columns]\n",
    "out_sorted = out[display_cols].sort_values(\"conf\", ascending=False)\n",
    "out_sorted.head(N_SAMPLES_QUAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"train\": {k: res_train[k] for k in [\"f1\",\"acc\",\"coverage\"]},\n",
    "    \"prod\":  {k: res_prod[k]  for k in [\"f1\",\"acc\",\"coverage\"]},\n",
    "    \"tau\": float(TAU),\n",
    "    \"recommended_tau_if_cov90\": summary_tau.get(\"recommended_tau_if_cov90\", None),\n",
    "    \"classes\": labels,\n",
    "}\n",
    "(REPORT_DIR / \"offline_summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
